---
title: "Preface"
author: "Michael Betancourt"
date: "February 2023"
date-format: "MMMM YYYY"
toc: true
number-sections: true
highlight: pygments
bibliography: preface.bib
format:
  html:
    html-math-method: katex
    theme:
      - lux
      - custom.scss
    embed-resources: true
    code-overflow: wrap
    linkcolor: "#B97C7C"
  pdf:
    keep-tex: true
    fig-width: 5.5
    fig-height: 5.5
    code-overflow: wrap
    monofontoptions:
      - Scale=0.5
knitr:
  opts_chunk:
    comment: ''
  opts_knit:
    global.par: TRUE
format-links: false
---

My formal education is in experimental particle physics.  I did take a 
theoretically-focused probability and statistics class that was mandated by my 
undergraduate curriculum, but my practical data analysis training was mostly
informed by heuristic techniques that I acquired in physics classes, labs, and 
research projects.

When I started my doctoral program I was assigned a thesis project that was 
expected to require a particularly careful analysis in order to separate out a 
weak signal from an overwhelming background.  One of the postdoctoral 
researchers in the group suggested that I look into this up-and-coming field 
known as "machine learning" and shared a few introductory tutorials.  To go 
beyond those initial pieces and some unsatisfying references I was able to find 
in the university library I asked some of my undergraduate classmates for 
recommendations.

One particularly productive recommendation pointed me to Chris Bishop's
"Pattern Recognition and Machine Learning" and David MacKay's 
"Information Theory, Inference, and Machine Learning".  Both of these books 
ended up being important stepping stones, but MacKay's textbook had something 
special.

One of my persistent frustrations with so many of the statistics and machine 
learning references I encountered was how distant the techniques they presented 
seemed to be from the experiments I was supposed to be analyzing.  While the 
heuristic methods I learned from my physics education were not always consistent 
nor well-behaved they at least tried to integrate models of both the experiment 
and the underlying physical systems.  The machine learning methods, however, 
were largely presented as black boxes that didn't need to acknowledge from where 
the data came.

The third chapter of "Information Theory, Inference, and Machine Learning" was 
a revelation.  Using an example he encountered in his own doctoral studies 
MacKay showed how we can use _Bayesian inference_ to learn from the the messy 
data that is not the exception but the rule in experimental physics, 
incorporating not only our understanding of a physical system but also the 
complications that often arise when trying to measure those systems into a 
consistent mathematical framework.

Unfortunately that initial excitement was dampened when MacKay never came back 
to this example in his book, instead focusing on more abstract examples.  I kept
searching but no references ever came as close to that applied ideal teased in 
Chapter 3.  For example many statistics and computer science references focused 
on formal results, considering models more for their mathematical convenience 
than any relevance to applied data analysis.  Others were encumbered by 
philosophical baggage, spending more time discussing what mathematics we should 
be allowed to apply in practice, or often which mathematical we _shouldn't_ be 
allowed to apply, without any mention of how to actually apply the mathematics 
that were being advocated.  Any discussion of realistic observations of actual, 
physical systems was largely limited to more applied treatments that, like what 
I had already encountered in experimental physics, used more heuristic modeling 
and inference methods that were often unreliable in practice.

After finishing my thesis I spent a year doing data analysis for a social media 
marketing company before moving to a postdoctoral position in statistics where 
some very patient colleagues helped me evolve beyond my heuristic education to 
a more theoretically grounded foundation in probability and statistics.  Along 
the way I found more and more exceptional references from authors like I.J. Good 
and George Box that, like MacKay, bridged theory and practice in productive 
ways.  I also had the privilege to collaborate on applied analyses in a broad 
set of fields, consistently evaluating the efficacy of data analysis techniques 
in those different environments.

This book is my attempt to distill these insights into a systematic framework 
for learning from one's data by modeling the particular complexities in how they 
might actually have been generated.  Such an ambitious task, however, does not 
come easily.  Understand this framework and how to apply it our own, unique 
applications will require investment in a long mathematical journey.

# Learning From Observations

If we hold an object in our hand and release it then it will fall towards the 
ground.  Releasing many balls from many hands establishes a persistent, 
qualitative pattern for how objects move under the influence of Earth's gravity.

To establish a more quantitative understanding of this phenomenon we to express 
our observations as data, and we need models for how that data could behave.  
For example we might record an object's exact height at various times during its 
fall (@fig-exact-data).  Our models would then need to capture different ways 
that an object could be released, in particular its initial height and velocity, 
and how it could behave after being released, in particular the trajectory it 
might trace out as it falls (@fig-exact-model).  The behavior of the falling 
object should, at least in theory, be quantified by the model that exactly 
reproduces the measurements (@fig-exact-fit).

::: {#fig-ideal layout="[ [-0.5, 33, 33, 33, -0.5] ]"}
![](figures/gravity/exact_data/exact_data){#fig-exact-data}

![](figures/gravity/exact_model/exact_model){#fig-exact-model}

![](figures/gravity/exact_fit/exact_fit){#fig-exact-fit}

In order to learn about how objects fall we have to compare (a) data to (b) 
models. (c) Ideally the behavior of falling objects should be quantified by the 
model that exactly reproduces the data.
:::

Unfortunately recording an object's height as it falls is easier said than done.  
Chaotic atmospheric forces imparted on the ball as it falls lead to momentary
excursions away from the expected trajectory.  At the same time we cannot 
resolve heights and times with infinite precision, limiting how well we can 
track an object as it falls.  Because of intricacies like these we will not, in 
general, be able to deterministically predict the outcome of a measurement even 
if we already understand the underlying behavior (@fig-realistic-data).  Instead 
our data will at best appear to fluctuate our the true model.

![Realistic measurements are unpredictable, at best fluctuating around some 
exact model.](figures/gravity/data/data){width=50% #fig-realistic-data}

These fluctuations complicate how we compare data to models.  With a 
sufficiently convenient fluctuation just about any model will be able to 
perfectly reproduce a given observation.  Consequently we will not be able to 
exclude any models outright.  Even if we constraint the fluctuation behavior  
many different models will be just as consistent with the observed data as any 
other model (@fig-consistent-configs).

::: {#fig-consistent-configs layout="[ [45, 45] ]"}
![](figures/gravity/data_consistent_config1/data_consistent_config1){#fig-consistent-config1}

![](figures/gravity/data_consistent_config3/data_consistent_config3){#fig-consistent-config3}

The lack of predictability in practical measurements complicates how we can 
learning from observations.  (a) Every model that is consistent with a noisy 
observation is accompanied by (b) many other models that are similarly 
consistent.
:::

Ultimately any insights we can extract from realistic data will be fundamentally
_uncertain_.  In order to learn from observations we will need to not only 
accept this uncertainty but also embrace it.

# Science

One way to formalize into a systematic process.

Consider, for example, learning about the environment around us through
experiment and observation, either for pure curiosity or to inform particular
decisions about how to interact with that environment.  The
**scientific method** organizes this learning process into a systematic
procedure (@fig-scientific-method-nominal).

![The scientific method reduces the process of learning from experiment and
observation into a sequence of basic steps, including one where we have to draw
inferences from observed data.](
figures/scientific_method/nominal/nominal){width=75% #fig-scientific-method-nominal}

While the basic steps of this procedure might appear straightforward, the
complexity of scientific inquiry is hidden in the details of their
implementation.  In particular when trying to implement the "Analyze Data" step
we have to confront the fundamental limitations in measurements and determine
how to quantify our inferential uncertainty.  Formally quantifying inferential
uncertainty is exactly the goal of **statistical inference**.

**There are many ways to implement each of these steps, and hence realize this
conceptual scientific method into an explicit process.  In particular there
are many ways to formalize "learning from data".  The most rigorous, however,
involve encoding our knowledge..."

To realize the "Analyze Data" step, and hence the scientific method as a whole,
we have to encode our knowledge into mathematical models, identify how
consistent those models are with a given observation, and then verify that the
consistent models adequately reproduce the structure of that observation
(@fig-scientific-method-box).  As George Box noted we cannot do rigorous science
without rigorous statistical modeling and inference [@Box:1976].

![Each step in the scientific method encapsulates many critical implementation
details.  For example in order to "Analyze Data" we need to be able to develop
candidate mathematical models, each representing one way that observations could
be generated, and then quantifying how consistent each of those models are with
an actual observation.](
figures/scientific_method/box/box){width=75% #fig-scientific-method-box}

# Bayesian Inference

**Somewhere around here reference Good's 17k types of Bayesians**
**The particular kind of Bayesian inference that we will consider in this
book.  Use of "principled" to describe acknowledging and checking our assumptions
instead of taking them for granted.**

In this book we will learn how to use **Bayesian inference** to analyze data
and, ultimately, implement the scientific method for both scientific and
industrial applications.  Bayesian inference uses **probability theory** to
not only develop **probabilistic models** of measurements but also to quantify
how consistent those models are with our domain expertise and observed data
(@fig-bayesian-configs).  **and verify those models**.

“Coherent degrees of belief”.  “Quantifying uncertainty”.
There are many pithy phrases used to motivate Bayesian inferences.  Conceptually these are all fine, but realizing them in practice is far from trivial.   We need to build meaningful models that are actually consistent with our understanding of the system being studied.  Then we need to be able to faithfully quantify the uncertainties that arise from that model.

::: {#fig-bayesian-configs layout="[ [45, 45] ]"}
![](figures/gravity/prior_ensemble/prior_ensemble){#fig-config_prior }

![](figures/gravity/posterior_ensemble/posterior_ensemble){#fig-config-posterior}

Bayesian inference uses probability theory to quantify (a) how a priori
consistent models are with any available domain expertise and then (b) how a
posteriori consistent they are with both the available domain expertise and any
observed data.
:::

Build models that incorporate not just the underlying phenomenon but also all of 
the ways that our measurements distort, contaminate, and otherwise corrupt our 
perceptions of that phenomenon.  Allowing us to learn from even complex data.

Although Bayesian inference is conceptually elegant its practical implementation
is far from trivial.  In order to implement a successful Bayesian analysis we
need to be proficient with not only building models but also critiquing their
adequacy in a particular application and accurately quantify their consistency
with observed data.  This books considers not only the conceptual foundations of
Bayesian inference but also these implementation challenges so that by its
conclusion you will be prepared to build the _bespoke_ analysis unique to your
particular application.

# Learning About Learning From Observations

To execute such an ambitious goal as painlessly as possible we need a careful
_pedagogical_ strategy.  What is the best way to learn how to robustly implement
Bayesian analyses?

In this section I'll discuss what I think is the best way to learn Bayesian
inference and hence why this book is structured so differently from many other
introductions to Bayesian analysis.

## Abstractions

Most subjects, including Bayesian inference are conceptually dense.  While they
might superficially appear simple the more closely we examine them the more
detail we need to confront in order to form a coherent theory.  In order to
make a subject approachable for students we have to rely on pedagogical
**abstractions** that hide the overwhelming depth behind only a selection of
concepts that are easier to digest.

A good abstraction focuses on simple concepts that are mostly consistent with
each other.  This consistency keeps the abstraction largely self-contained,
obscures the hidden depth below.  Scrutinizing the inconsistencies that do
manifest in a given abstraction, however, will eventually lead us beyond the
boundaries of that abstraction and into the domain of a new, deeper abstraction.
When developing practical methodologies we need to find a sufficiently rich
abstraction that captures all of the necessary concepts without too much
unnecessary detail.

Consider, for example, the real numbers.  At a high-level we can conceptualize
the real numbers as a _continuum_ with infinite detail no matter how closely we
zoom in.  This is a completely reasonable abstraction for most applications
involving not only basic arithmetic but also more complicated operations like
differentiation and integration.  When we want to engage in more technical
results, however, we will need to go beyond this simple abstraction in order to
develop a consistent theory free of pathological behavior.

Abstractions also play a role in how real numbers are implemented in practice.
At a high-level we can assume that computers are able to exactly represent real
numbers and exactly implement their arithmetic operations.  In many applications
this abstraction is entirely valid.  When working on applications that require
precise or complicated calculations, however, we start to see cracks in this
conceptual picture.

Eventually we have to confront the reality of the _finite precision arithmetic_
implemented on computers and how it deviates from exact arithmetic.  Going
slightly deeper we might consider the limited dynamic range of _floating point_
numbers that can result in underflow and overflow for particularly small and
large results.  Digging even further we might tackle how underflow and overflow
in intermediate calculations can lead to catastrophic errors even if the final
result is not itself extremely small or large.

## Sequences of Abstractions

In most applications an abstraction that is sufficiently detailed is too
overwhelming to approach all at once.  Instead we have to progress towards it
carefully through a sequence of intermediate abstractions.  This leaves us to
consider the progressions that best guide students towards that terminal
abstraction.

### Non-Overlapping Progressions

One approach, for example, utilizes a progression of _non-overlapping_
abstractions (@fig-abs-incons).  This allows the intermediate abstractions to
incorporate compelling intuitions and examples even if they don't generalize to
the final abstraction.  Moreover each abstraction can be compartmentalized into
a relatively self-contained course, and different students with different goals
can follow the progression to different terminal abstractions.  For these
reasons this approach is particularly common in academia.

::: {#fig-abs-incons layout="[ [33, 33, 33] ]"}
![](figures/pedagogical_abstractions/inconsistent/1/1){#fig-abs-incons1 }

![](figures/pedagogical_abstractions/inconsistent/2/2){#fig-abs-incons2}

![](figures/pedagogical_abstractions/inconsistent/3/3){#fig-abs-incons3}

Non-overlapping progressions build up to a final, sufficient abstraction through
inconsistent intermediate abstractions.  At each iteration only some of the
concepts are carried forward to the new, more detailed abstraction.
:::

Non-overlapping progressions can be problematic, however, when the sequence of
intermediate abstractions terminates prematurely.  The insights of the last
intermediate abstraction encountered might be useful in some circumstances, but
without the context of the remaining abstractions a student will likely have
difficulty identifying exactly what those necessary circumstances, let alone
validating when they hold in a given application.  In other words the
understanding offered by only an intermediate abstraction can be _fragile_ when
the assumptions holding that abstraction together cannot be taken for granted.

Another problem with this approach is that the updating from one abstraction to
the next can be burdensome on students.  Because the abstractions don't overlap
a student doesn't just expand their understanding with new concepts at each
iteration; instead they also have to _unlearn_ the concepts that don't
generalize from the previous abstraction (@fig-unlearning).  This repeated cycle
of learning and unlearning can be frustrating and even discourage students from
moving past an intermediate, and potentially fragile, abstraction.

::: {#fig-unlearning layout="[ [-33, 33, -33], [33, 33, 33]]"}
![](figures/pedagogical_abstractions/unlearning/1/1){#fig-unlearning1}

![](figures/pedagogical_abstractions/unlearning/2/2){#fig-unlearning2}

![](figures/pedagogical_abstractions/unlearning/3/3){#fig-unlearning3}

![](figures/pedagogical_abstractions/unlearning/4/4){#fig-unlearning4}

The (a) inconsistent abstractions in a non-overlapping progression can frustrate
learning.  At each iteration (b) only some concepts will generalize and transfer
from one abstraction to the next and (c) the newer abstraction will also
introduce new concepts that have to be learned.  (d)  Some concepts in the
initial abstraction, however, will not generalize.  Students have to actively
_unlearn_ these concepts in order to fully grasp the newer abstraction.
:::

For example a non-overlapping approach towards teaching arithmetic on computers
might start by assuming that computers implement arithmetic exactly.  In this
case the initial abstraction can demonstrate implementation with simple programs
that define real variables and evaluate arithmetic operations with negligible
error.  Students can then use these initial insights to write new programs that
can yield reasonable results in many cases but might exhibit large, often
ignored errors in others.  A following abstraction might introduce programs that
explicitly demonstrate errors before introducing the practical reasons why real
numbers and their basic operations can't be exactly implemented on computers
with finite resources.  The progression could then continue to abstractions that
introduce the basic structure of fixed-point and floating-point arithmetic and
their pitfalls before presenting the technical details of fixed-point and
floating-point arithmetic implementations on contemporary computers.

**conceptual debt in analogy with technical debt.  Some debt is useful for
facilitating progress but too much debt can make later steps overwhelming**

### Overlapping Progressions

Alternatively we can build up to that final, sufficient abstraction with a
progression of _overlapping_ abstractions (@fig-abs-cons).  By avoiding concepts
that don't persist to the final abstraction entirely nothing has to be
unlearned, and students can instead focus on expanding their understanding to
the new concepts introduced at each iteration.

::: {#fig-abs-cons layout="[ [33, 33, 33] ]"}
![](figures/pedagogical_abstractions/consistent/1/1){#fig-abs-cons1}

![](figures/pedagogical_abstractions/consistent/2/2){#fig-abs-cons2}

![](figures/pedagogical_abstractions/consistent/3/3){#fig-abs-cons3}

Overlapping progressions build up to a final, sufficient abstraction through
consistent intermediate abstractions.  At each iteration all of the concepts are
carried forward to the new, more detailed abstraction avoiding any need to
unlearn deprecated concepts.
:::

Unfortunately the concepts that don't generalize often provide more explicit,
compelling motivation than the concepts that do generalize.  Because of this
intermediate abstractions along an overlapping progression can be difficult to
relate to the ultimate objectives.  At the same time the persistent concepts
alone, and hence the intermediate abstractions they shape, often appear
incomplete, at least until the progression approaches the final abstraction.
Consequently to benefit from a non-overlapping approach students have to be
sufficiently dedicated to persevere through the early, less concrete
abstractions.

For example an overlapping approach to teaching computer arithmetic might start
not with any demonstrative code but rather a discussion of the infinite memory
it would take to store an arbitrary real number, the infinite processing power
it would take to evaluate arithmetic for arbitrary real numbers.  A subsequent
abstraction might consider the various ways that we can theoretically
approximate real numbers and their operations with only finite memory and finite
processing power.  Without explicit implementations, and code demonstrating
those implementations, there is nothing that students can abuse but also nothing
with which students can experiment and build their comprehension.  Only in later
abstractions where we see how finite-precision arithmetic is implemented will
we be able to construct interactive examples.

## Why This Book Is So Long

I think that it's fair to say that most pedagogical resources take a
non-overlapping approach to teaching statistics in general, let alone Bayesian
inference in particular.  In order to reach demonstrative examples as soon as
possible these resources typically begin with simple models and little if any
critique of the assumptions implied by those models.  At the same time the
construction of inferences from these models is often delegated to tools whose
accuracy is taken for granted.  Few resources move beyond these relatively
shallow abstractions leaving students unaware at how fragile some of the
insights might be.

Many of these resources inspire enthusiasm for Bayesian inference which, and I
mean this sincerely, cannot be discounted.  Enthusiasm alone, however, may not
be enough for students to avoid the consequences of fragile insights.  Many
students struggle to connect the simple models of shallow abstractions to their
own applications.  Some identify the problematic consequences of applying
simple models when they're not appropriate but, without a broader context,
mistakenly blame their own implementation of those models and not the models
themselves.

This book is for those students.  I embrace an overlapping progression to
deliberately, if slowly, develop the probabilistic tools needed to build
_bespoke_ models appropriate to a particular application and implement
_faithful_ Bayesian inference that accurately quantifies uncertainty for those
models.  Our goal is artisanal models, not something mass produced and sold at
big box stores.

Depending on your previous experience with probabilistic modeling and Bayesian
inference you may find useful insights by jumping straight into later chapters.
That said this book is designed to be read from beginning to end as concepts,
terminology, and mathematical notation are all built up progressively from the
beginning.  Starting from the beginning will also help you confront and unlearn
any misconceptions about probability, modeling, and inference that you may have
picked up along your journey.

I do not assume any prior knowledge of the theory or practical implementation of
probability theory, probabilistic modeling, or Bayesian inference, but I do
assume some basic mathematical experience.  In particular the book will require
a conceptual understanding of calculus, namely the basic theory and
implementation of differentiation and integration, as well as working knowledge
of linear algebra.

Most introductory calculus textbooks, such as @LarsonEtAl:2005 and
@Stewart:2015, will cover the relevant concepts.  More sophisticated treatments
like @Apostol:1967 and @Apostol:1969 aren't necessary but can be helpful
references when looking into some more subtle technical details.  For linear
algebra @TrefethenEtAl:1997 is particularly thorough about not just the basic
of linear algebra but also the common pitfalls of its practical implementation.

Later chapters introduce code demonstrations in `R` and `Python`.  If you are
comfortable with either language then I encourage you to not just look through
the code but also run it yourself.  Playing around with these code
demonstrations is a powerful way to develop and reinforce your comprehension.

The Carpentries offer a
[variety of workshops](https://carpentries.org/workshops-curricula/#swc-all)
that introduce both `R` and `Python`.  Jenny Bryant's
[Stat545 course material](https://stat545.com) is another great resource for
introducing oneself to `R` and @Sweigart:2019 is a nice resource for learning
`Python`.

# This Book

With all of that said let's look at the pedagogical progression of this book in
a bit more detail.  The book is organized into three parts: the first focuses
on applied probability theory, the second on the general principles of
probabilistic modeling and statistical inference, and the third on particular
modeling techniques.

In Part I we will learn the mathematical properties of probability
distributions, the operations that we can use to manipulate probability
distributions, and some of the most useful methods for approximately
implementing those operations in practice.  The first few chapters will
necessarily be a bit abstract until we can properly set up those
implementations but your patience will be rewarded.  Overall these chapter will
go into much more detail about probability theory than most introductions to
Bayesian inference, although that that detail will focus on conceptual
understanding and practical insights rather than technical formalities and
proofs.

A thorough understanding of applied probability theory sets the stage for Part
II where we will use probability distributions to quantify unpredictable
measurements in theory, approximately model the processes which give rise to
measurements in practice, and quantify our uncertainty about how consistent
various approximations are with the outcome of a specific measurement.  In
particular we will focus on techniques to translate our understanding of a
system and measurements that interrogate that system into bespoke probabilistic
models capable to gleaming precise insights from data.

With that conceptual foundation established Part III considers particular
modeling techniques that can be useful as modular building blocks for developing
these bespoke models.  Each chapter focuses on not only the assumptions inherent
to a given technique and how to validate those assumptions but also on efficient
implementations.  Many of the chapters will consider popular estimation
techniques from the modeling perspective we need to rigorously integrate them
into Bayesian analyses.

Although Part III introduces many modeling techniques it is by no means
exhaustive.  One of the exciting aspects of probabilistic modeling is an
eternal opportunity to learn new techniques, expanding our modeling toolkit and
the sophistication of bespoke models that we can employ in practice.  This book
will hopefully prepare you for that never-ending, but never-boring journey.

# About Me?

Physics analyses using heuristic methods.

McKay's mention of modeling a broken particle detector.  Alas only a tease.
This book is largely an attempt to fill in that gap and provide the tools
necessary for building analyses around interpretable models that take
advantage of any available domain expertise.


# Acknowledgements

I thank Ero Carrera, Ramiro Barrantes-Reynolds, and Edgar Merkle for helpful
comments.

<!--
Many people to thank for giving me the opportunity to not only learn this
material myself but also share it with you.
-->

# License {-}

A repository containing all of the files used to generate this chapter is
available on [GitHub](
https://github.com/betanalpha/quarto_chapters/tree/main/preface).

The text and figures in this chapter are copyrighted by Michael Betancourt
and licensed under the CC BY-NC 4.0 license:

https://creativecommons.org/licenses/by-nc/4.0/
